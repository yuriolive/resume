---
name: YURI OLIVE
label: Lead Data Engineer | AWS & Azure Certified | McKinsey Alumni
email: ""
phone: ""
url: https://yuriolive.com
profiles:
  - network: linkedin
    username: yuriolive
    url: https://www.linkedin.com/in/yuriolive
summary: |
  Lead Data Engineer with over 8 years of experience architecting scalable data pipelines and AI/ML solutions. Proven track record of managing data infrastructures for organizations of up to **[...]** employees and handling data volumes exceeding **[...]** TB. Expert in modernizing ELT frameworks and cloud-native architectures to drive a **[...]%** increase in operational efficiency.

location:
  city: "Jundiaí"
  countryCode: BR
  region: "São Paulo"
  country: "Brazil"

skills:
  - name: Programming Languages
    keywords: [Python, Scala, TypeScript, SQL, Java, VB.NET]
  - name: Databases & Storage
    keywords:
      [Snowflake, BigQuery, Redshift, MongoDB, Cassandra, PostgreSQL, MS SQL]
  - name: Data Tools
    keywords:
      [Airflow, dbt, Meltano, Kafka, Spark, Databricks, Scrapy, Terraform]
  - name: Cloud
    keywords:
      [
        AWS (Solutions Architect),
        Microsoft Azure (Data Engineer),
        Google Cloud Platform (GCP),
      ]
  - name: ML Frameworks & Tools
    keywords: [Kedro, Spark MLLib, LangGraph, Pytest]
  - name: Others
    keywords:
      [Docker, Kubernetes, GitHub Actions, CI/CD, Prisma, Apollo GraphQL]

education:
  - institution: UNICAMP - Universidade Estadual de Campinas
    area: BSc, Computer Engineering
    studyType: Bachelor
    startYear: 2013
    endYear: 2018

certificates:
  - name: AWS Certified Solutions Architect – Associate
  - name: Azure Data Engineer Associate (Microsoft)
  - name: Modernizing Data Lakes and Data Warehouses with GCP (Coursera)

# Experience will be parsed from the Markdown below
---

# EXPERIENCE

## Lead Data Engineer @ Correlation One

_April 2023 – Present_

- **Spearheaded** the redesign of legacy data pipelines using Meltano, Scrapy, and Airflow, resulting in a **[...]%** reduction in infrastructure maintenance time and a **[...]%** improvement in data reliability.
- **Architected** a custom data extraction solution for Slack to bypass 2FA and high-cost API limitations, saving the company **[...] USD** per month in operational costs.
- **Developed** a robust data pipeline for SMA Resume collection, implementing request-bypass logic that successfully processed **[...]** resumes for the recruitment team without hitting service limits.
- **Integrated** the Expert App and Grading Service databases into a centralized BigQuery warehouse, enabling **[...]** internal stakeholders to access real-time performance metrics via dbt.
- **Standardized** production environments by implementing Infrastructure-as-Code (Terraform) and CI/CD (GitHub Actions), which reduced deployment failure rates by **[...]%**.

## Staff Data Engineer @ Authority.Org

_December 2021 – February 2023_

- **Led** the construction of an end-to-end data pipeline using dbt and Snowflake, consolidating **[...]** disparate data sources into a single source of truth for university evaluations.
- **Orchestrated** complex workflows using Airflow and Kubernetes, supporting a network of **[...]** context-rich websites and improving their data-refresh frequency by **[...]%**.
- **Improved** Google organic ranking for primary web assets by implementing a dimensionally modeled data structure that served context-rich metadata via a TypeScript GraphQL API.

## Lead Data Engineer @ TRACTIAN

_July 2021 – December 2021_

- **Restructured** AWS cloud infrastructure and security using Terraform for **[...]** active sensors, preventing an estimated **[...]** hours of potential downtime.
- **Decoupled** Python and NodeJS microservices using Apache Kafka (AWS MSK), enabling real-time processing of **[...]** events per second for predictive maintenance ML models.

## Data Engineer @ McKinsey & Company

_February 2020 – July 2021_

- **Optimized** an e-commerce recommendation pipeline using Databricks (Scala/Spark) for a major retailer, resulting in a **1.8%** increase in client revenue, totaling **[...] USD**.
- **Engineered** a recommendation platform for a global shrimp producer using Kedro and GCP, increasing average product size by **[...]%** while reducing feed costs by **[...]%**.
- **Resolved** critical production data issues for an industrial vessel fleet, ensuring 100% on-time delivery of data reports and avoiding potential financial penalties of **[...] USD**.

## Data Specialist @ Stone

_May 2019 – February 2020_

- **Constructed** a scalable BigQuery Data Lake, reducing average query execution times from several days to **[...]** minutes for the finance department.
- **Migrated** a legacy SQL Server warehouse to Snowflake, moving **[...] TB** of financial data while applying Kimball principles to improve cross-departmental reporting speed by **[...]%**.

## Data Engineer @ Big Data Brasil

_June 2018 – May 2019_

- **Engineered** a serverless data acquisition infrastructure on AWS (Lambda, SQS, S3) using Infrastructure-as-Code (CloudFormation), supporting the secure ingestion of **[...]** million records daily.
- **Refactored** high-availability Python (Scrapy) web scrapers, increasing data collection success rates by **[...]%** across thousands of distinct public and private data sources.

## Software Developer Intern / DBA @ Dolphin Enterprises Ltd

_September 2017 – May 2018_

- **Optimized** MS SQL Server stored procedures and migrated on-premise databases to **Azure SQL Elastic Pools**, resulting in a **[...]%** improvement in application responsiveness.
- **Developed** a remote investor portal using **VB.NET (ASP.NET MVC 5)** and Entity Framework, providing secure data access to **[...]** external stakeholders.

## Software Engineer Intern @ Odysci

_June 2015 – August 2015_

- **Developed** Machine Learning algorithms for sentiment analysis and e-commerce search using **Java**, improving search relevance for user-generated content by **[...]%**.
- **Automated** social media monitoring workflows for the Odysci Media Analyzer platform, reducing manual data processing time by **[...]** hours per week.
